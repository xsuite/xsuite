# This is a test workflow the other (self-hosted runner based) workflows rely on
name: Test workflow (self-hosted)

# Controls when the action will run. This is a reusable workflow.
on:
  workflow_call:
    # Inputs the workflow accepts.
    inputs:
      locations:
        description: 'Branch locations (JSON array string)'
        default: '{"xobjects_branch":"xobjects:main",
        "xdeps_branch":"xdeps:main" ,
        "xpart_branch":"xpart:main" ,
        "xtrack_branch":"xtrack:main" ,
        "xfields_branch":"xfields:main" ,
        "xmask_branch":"xmask:main" ,
        "xcoll_branch":"xcoll:main" ,
        "xboinc_branch":"xboinc:main" }'
        required: true
        type: string
      test_contexts:
        required: false
        type: string
        default: 'ContextCpu;ContextCpu:auto;ContextCupy;ContextPyopencl'
      platform:
        required: true
        type: string
      suites:
        description: a list of the suites to run as a JSON string
        required: false
        type: string
        default: '["xobjects", "xdeps", "xpart", "xtrack", "xfields", "xmask", "xcoll","xboinc"]'

env:
  with_gpu: ${{ contains(inputs.test_contexts, 'Cupy') || contains(inputs.test_contexts, 'Pyopencl') }}
  JSON_INPUT: ${{ github.event.inputs.locations }}
# A workflow run is made up of one or more jobs that can run sequentially or in parallel
# The jobs are all run in independent environments. Here we will run a separate job
# for each of the test suites specified in the matrix below.
jobs:
  # First, we build our test image
  build-test-image:
    runs-on: [self-hosted, "${{ inputs.platform }}"]
    outputs:
      image_id: ${{ steps.build-image.outputs.image_id }}
    steps:
      - name : Install jq
        run: |
          if command -v apt-get &> /dev/null; then
            sudo apt-get update && sudo apt-get install -y jq
          elif command -v yum &> /dev/null; then
            sudo yum install -y jq
          elif command -v apk &> /dev/null; then
            sudo apk add --no-cache jq
          else
            echo "Package manager not supported"
            exit 1
          fi

      - id: checkout-repo
        name: Checkout the repo
        uses: actions/checkout@v3
      - id: build-image
        name: Set branch Environment Variable
        # JSON_INPUT: ${{ github.event.inputs.locations }}
      #  env:  
          
      #     echo -e "xobjects_branch = $( echo $JSON_INPUT | jq ".xobjects_branch")" >> $GITHUB_ENV
      #     echo -e "xdeps_branch = $( echo $JSON_INPUT | jq ".xdeps_branch")" >> $GITHUB_ENV
      #     echo -e "xpart_branch = $( echo $JSON_INPUT | jq ".xpart_branch")" >> $GITHUB_ENV
      #     echo -e "xtrack_branch = $( echo $JSON_INPUT | jq ".xtrack_branch")" >> $GITHUB_ENV
      #     echo -e "xfields_branch = $( echo $JSON_INPUT | jq ".xfields_branch")" >> $GITHUB_ENV
      #     echo -e "xmask_branch = $( echo $JSON_INPUT | jq ".xmask_branch")" >> $GITHUB_ENV
      #     echo -e "xcoll_branch = $( echo $JSON_INPUT | jq ".xcoll_branch")" >> $GITHUB_ENV
      #     echo -e "xboinc_branch = $( echo $JSON_INPUT | jq ".xboinc_branch")" >> $GITHUB_ENV
        #  xpart_branch: ${{ fromJson(inputs.locations).xpart_branch }}
        #  xtrack_branch: ${{ fromJson(inputs.locations).xtrack_branch }}
        #  xfields_branch: ${{ fromJson(inputs.locations).xfields_branch }}
        #  xmask_branch: ${{ fromJson(inputs.locations).xmask_branch }}
        #  xcoll_branch: ${{ fromJson(inputs.locations).xcoll_branch }}
        #  xboinc_branch: ${{ fromJson(inputs.locations).xboinc_branch }}
      #   - name: Build and Deploy 
        run: |
# echo "xobjects_branch = $( echo $JSON_INPUT | jq -r ".xobjects_branch")
          # xdeps_branch = $( echo $JSON_INPUT | jq -r ".xdeps_branch")
          # xpart_branch = $( echo $JSON_INPUT | jq -r ".xpart_branch") 
          # xtrack_branch = $( echo $JSON_INPUT | jq -r ".xtrack_branch") 
          # xfields_branch = $( echo $JSON_INPUT | jq -r ".xfields_branch") 
          # xmask_branch = $( echo $JSON_INPUT | jq -r ".xmask_branch") 
          # xcoll_branch = $( echo $JSON_INPUT | jq -r ".xcoll_branch")          
          # xboinc_branch = $( echo $JSON_INPUT | jq -r ".xboinc_branch") 

          echo -e "xobjects_branch = $( echo $JSON_INPUT | jq ".xobjects_branch")" >> $GITHUB_ENV
          echo -e "xdeps_branch = $( echo $JSON_INPUT | jq ".xdeps_branch")" >> $GITHUB_ENV
          echo -e "xpart_branch = $( echo $JSON_INPUT | jq ".xpart_branch")" >> $GITHUB_ENV
          echo -e "xtrack_branch = $( echo $JSON_INPUT | jq ".xtrack_branch")" >> $GITHUB_ENV
          echo -e "xfields_branch = $( echo $JSON_INPUT | jq ".xfields_branch")" >> $GITHUB_ENV
          echo -e "xmask_branch = $( echo $JSON_INPUT | jq ".xmask_branch")" >> $GITHUB_ENV
          echo -e "xcoll_branch = $( echo $JSON_INPUT | jq ".xcoll_branch")" >> $GITHUB_ENV
          echo -e "xboinc_branch = $( echo $JSON_INPUT | jq ".xboinc_branch")" >> $GITHUB_ENV
         
          IMAGE="xsuite-test-runner-$(cat /proc/sys/kernel/random/uuid)"
          echo "image_id=$IMAGE" >> $GITHUB_OUTPUT
          docker build \
            --network=host \
            --no-cache=true \
            --build-arg xobjects_branch=$xobjects_branch  \
            --build-arg xdeps_branch=$xdeps_branch \
            --build-arg xpart_branch=$xpart_branch \
            --build-arg xtrack_branch=$xtrack_branch \
            --build-arg xfields_branch=$xfields_branch \
            --build-arg xmask_branch=$xmask_branch \
            --build-arg xcoll_branch=$xcoll_branch \
            --build-arg xboinc_branch=$xboinc_branch \ 
            --build-arg with_gpu=${{ env.with_gpu }} \
            -t $IMAGE .


  # Print out some stuff about the test environment
  image-sanity-checks:
    runs-on: [self-hosted, "${{ inputs.platform }}"]
    needs: build-test-image
    env:
      image_id: ${{ needs.build-test-image.outputs.image_id }}
    steps:
      - name: CUDA/ROCm info
        if: ${{ env.with_gpu == 'true' }}
        run: docker run --rm --gpus all ${image_id} bash -c "nvidia-smi || rocm-smi"
      - name: OpenCL info
        if: ${{ env.with_gpu == 'true' }}
        run: docker run --rm --gpus all ${image_id} clinfo
      - name: Package paths
        run: docker run --rm --gpus all ${image_id} python3 /opt/xsuite/xtrack/examples/print_package_paths.py
      - name: List dependencies
        run: docker run --rm --gpus all ${image_id} pip freeze

  # Run the tests for each repo in parallel in a test container
  run-tests:
    runs-on: [self-hosted, "${{ inputs.platform }}"]
    needs: [build-test-image, image-sanity-checks]
    timeout-minutes: 540
    strategy:
      fail-fast: false
      matrix:
        test-suite: ${{ fromJson(inputs.suites) }}

    steps:
    - name: Run pytest
      env:
        image_id: ${{ needs.build-test-image.outputs.image_id }}
        test_contexts: ${{ inputs.test_contexts }}
      run: |
        mkdir -p reports/${{ matrix.test-suite }}
        docker run --rm --gpus all \
          --env XOBJECTS_TEST_CONTEXTS="${test_contexts}" \
          -v $PWD/reports/${{ matrix.test-suite }}:/opt/reports:Z \
          ${image_id} \
          /opt/run_tests.sh /opt/xsuite/${{ matrix.test-suite }}/tests

  wait:
    runs-on: [self-hosted, "${{ inputs.platform }}"]
    needs: [build-test-image, run-tests]
    if: success() || failure()
    steps:
      - name: Wait
        run: sleep 300

  # Cleanup after the tests by removing the image and making sure there are
  # no unused images and stopped containers
  teardown:
    runs-on: [self-hosted, "${{ inputs.platform }}"]
    needs: [build-test-image, wait]
    env:
      image_id: ${{ needs.build-test-image.outputs.image_id }}
    if: always()
    steps:
      - name: Stop the containers and remove the image
        run: |
          docker container stop \
            $(docker ps -q --filter ancestor=${image_id}) || true
          docker container rm --volumes \
            $(docker ps -qa --filter ancestor=${image_id}) || true
          docker image rm ${image_id}
